{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500b4c09",
   "metadata": {
    "id": "500b4c09"
   },
   "source": [
    "### mac gpu 사용가능확인 (1이면 사용가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6DPP8CTKbreN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DPP8CTKbreN",
    "outputId": "84255d93-1879-4dfa-ed93-f711a1d51765"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7af62bc",
   "metadata": {
    "id": "d7af62bc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9aa630",
   "metadata": {
    "id": "ea9aa630",
    "outputId": "6066603e-26bb-4a41-812b-6743602d2675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9bc0",
   "metadata": {
    "id": "f34e9bc0"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5563b4c6",
   "metadata": {
    "id": "5563b4c6"
   },
   "outputs": [],
   "source": [
    "# tensorflow.compat.v2 를 사용\n",
    "import tensorflow.compat.v2 as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca3837",
   "metadata": {
    "id": "9fca3837"
   },
   "source": [
    "# dataset\n",
    "> 몇십만개의 행의 csv 파일들(몇십GB용량)을 kaggle에서는 주었는데\n",
    "> git 업로드 문제로 10000개행만 따로 추출한 csv파일을 사용\n",
    "  \n",
    "> ram문제로 일단 10개만 추출해서 학습데이터로 사용해보려함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cc5ae5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "03cc5ae5",
    "outputId": "0d13f27d-9ba3-47b1-d90d-98bd82a9e0c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(863, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>image_url</th>\n",
       "      <th>caption_title_and_reference_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>Downtown Deer Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>Jürgen Ovens's Justitia, 1663-1665, Museumsber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>1956 MV Agusta 250 Raid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>Seth MacFarlane's logo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>en</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>Erskine River at Lorne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                          image_url  \\\n",
       "2        en  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "5        en  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "11       en  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "23       en  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "38       en  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "\n",
       "              caption_title_and_reference_description  \n",
       "2                                  Downtown Deer Park  \n",
       "5   Jürgen Ovens's Justitia, 1663-1665, Museumsber...  \n",
       "11                            1956 MV Agusta 250 Raid  \n",
       "23                             Seth MacFarlane's logo  \n",
       "38                             Erskine River at Lorne  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wiki_small_data.csv', delimiter = ',', usecols=['language','image_url', 'caption_title_and_reference_description'])\n",
    "df = df[df['language']=='en'] # 일단 영어에 대해서만 진행\n",
    "p = re.compile('\\[SEP\\].+')\n",
    "df['caption_title_and_reference_description'] = df['caption_title_and_reference_description'].apply(\n",
    "                                lambda x: p.search(x).group().replace('[SEP] ', '') \n",
    "                                if p.search(x).group() not in['[SEP] ', ''] else None)\n",
    "df=df.dropna(axis=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d834668b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d834668b",
    "outputId": "14b6abcf-3344-4160-ac01-a49a43fe7629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/commons/2/28/Deer_Park_Wisconsin_Downtown_WIS46.jpg\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['image_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76d1277",
   "metadata": {
    "id": "c76d1277"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "def url_to_image(url):\n",
    "    '''\n",
    "    url 에서 이미지를 추출하여, (512,512,3) 의 rgb ndarray로 리턴\n",
    "    '''\n",
    "    resp = request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)/255.0\n",
    "    return cv2.resize(image, (512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d96f146",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d96f146",
    "outputId": "a6ee22f6-855a-4c8e-cbb6-c5dc19229ea6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(512, 512, 3)\n",
      "9\n",
      "Downtown Deer Park\n"
     ]
    }
   ],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for i,j in enumerate((df.iloc[:10]['image_url'])):\n",
    "    try:\n",
    "      X_train.append(url_to_image(j))\n",
    "      y_train.append( df.iloc[i]['caption_title_and_reference_description'] )\n",
    "    except:\n",
    "      pass\n",
    "print(len(X_train))\n",
    "print(X_train[0].shape)\n",
    "print(len(y_train))\n",
    "print(y_train[0])\n",
    "\n",
    "# png 파일일 경우 libpng경고가 나오는데 무시해도 좋을듯하다.\n",
    "# srv 파일의 경우 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3934e7",
   "metadata": {
    "id": "ba3934e7"
   },
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a01efa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14a01efa",
    "outputId": "0ac09a49-6d11-4c56-8e09-f40eca806153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24, 25, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train 은 캡션 문장인데, 토크나이저를 통해 문장들을 단어별 벡터화 해준다. (cap_vector)\n",
    "\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&*+.-;?@[]^`{}~ ')\n",
    "tokenizer.fit_on_texts(y_train)\n",
    "train_seqs = tokenizer.texts_to_sequences(y_train)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "print(len(cap_vector[0]))\n",
    "cap_vector[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e22ce3",
   "metadata": {
    "id": "e5e22ce3"
   },
   "source": [
    "# 전역변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70c3d07c",
   "metadata": {
    "id": "70c3d07c"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(X_train) // BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aee54e",
   "metadata": {
    "id": "d5aee54e"
   },
   "source": [
    "# optimizer, loss func.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95481c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d589303",
   "metadata": {
    "id": "1d589303"
   },
   "source": [
    "# model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db640294",
   "metadata": {
    "id": "db640294"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    '''\n",
    "    W1, W2, V 는 학습가능한 가중치벡터\n",
    "    '''\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    '''\n",
    "    feautures : 이미지의 피쳐맵\n",
    "    hidden : hidden state ( )\n",
    "    '''\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27003f76",
   "metadata": {
    "id": "27003f76"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "n6zPA3vDezmt",
   "metadata": {
    "id": "n6zPA3vDezmt"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    x = self.fc1(output)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    x = self.fc2(x)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28da6c1",
   "metadata": {
    "id": "d28da6c1"
   },
   "source": [
    "# trian step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cca31e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 21:27:42.025145: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-28 21:27:42.026399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ad928",
   "metadata": {
    "id": "921ad928"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    '''\n",
    "    배치 한번에 대한 학습과정을 커스터마이징\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 모델사용 예측 (prediction)\n",
    "        predictions = model(x)\n",
    "        # 2. Loss 계산\n",
    "        loss = loss_function(y, predictions)\n",
    "    \n",
    "    # 3. 그라디언트(gradients) 계산\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 4. 오차역전파(Backpropagation) - weight 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # loss와 accuracy를 업데이트 합니다.\n",
    "    train_loss(loss)\n",
    "    train_acc(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d691eb",
   "metadata": {
    "id": "06d691eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
