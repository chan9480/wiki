{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500b4c09",
   "metadata": {
    "id": "500b4c09"
   },
   "source": [
    "### mac gpu 사용가능확인 (1이면 사용가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Akm7USgsUzRK",
   "metadata": {
    "id": "Akm7USgsUzRK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9aa630",
   "metadata": {
    "id": "ea9aa630",
    "outputId": "6066603e-26bb-4a41-812b-6743602d2675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6DPP8CTKbreN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DPP8CTKbreN",
    "outputId": "3f067f74-9d81-43b7-8e91-5d30aadbe003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e9bc0",
   "metadata": {
    "id": "f34e9bc0"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5563b4c6",
   "metadata": {
    "id": "5563b4c6"
   },
   "outputs": [],
   "source": [
    "# tensorflow.compat.v2 를 사용\n",
    "import tensorflow.compat.v2 as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca3837",
   "metadata": {
    "id": "9fca3837"
   },
   "source": [
    "# dataset\n",
    "> 각 train 데이터 ( train_1 ~ train_10) 는 small_data.ipynb 에서 따로 처리하여 만들어짐.  \n",
    "> 영어만 추출, image_url 과 caption_feature 만 추출.\n",
    "\n",
    "출처 : https://www.kaggle.com/c/wikipedia-image-caption/code?competitionId=29705&searchQuery=tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "03cc5ae5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "03cc5ae5",
    "outputId": "8df4cfbf-6ddb-4f07-8914-a4686b7d1bc8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301484, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>caption_title_and_reference_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>&lt;start&gt; downtown deer park &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>&lt;start&gt; jürgen ovens's justitia, -, museumsber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>&lt;start&gt;  mv agusta  raid &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>&lt;start&gt; seth macfarlane's logo &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>&lt;start&gt; erskine river at lorne &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "1  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "2  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "4  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "6  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "\n",
       "             caption_title_and_reference_description  \n",
       "0                   <start> downtown deer park <end>  \n",
       "1  <start> jürgen ovens's justitia, -, museumsber...  \n",
       "2                     <start>  mv agusta  raid <end>  \n",
       "4               <start> seth macfarlane's logo <end>  \n",
       "6               <start> erskine river at lorne <end>  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caption_feature를 보면\n",
    "# 1. [SEP] 으로 나뉘어져 있는데, 그 뒤에 있는 내용에 대해 할 것. (BERT)에서 사용하는 스페셜 토큰인데 파인튜닝은 안할 것이므로..\n",
    "# 2. 숫자정보는 제외하자 대부분 특수한 경우에 쓰이거나 주소 등이다.\n",
    "\n",
    "df = pd.read_csv('train/train_1.tsv', delimiter = '\\t')\n",
    "p = re.compile('\\[SEP\\].+') # \\로 감싸진 곳은 \n",
    "df['caption_title_and_reference_description'] = df['caption_title_and_reference_description'].apply(\n",
    "                                lambda x: '<start> ' + \n",
    "                                        re.sub('\\d+', '', p.search(x).group().replace('[SEP] ', '')).lower() \n",
    "                                        +' <end>'\n",
    "                                        if p.search(x).group() not in['[SEP] ', ''] else None)\n",
    "df=df.dropna(axis=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sVm2l0QnXe9k",
   "metadata": {
    "id": "sVm2l0QnXe9k"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "def url_to_image(url):\n",
    "    '''\n",
    "    url 에서 이미지를 추출하여, (512,512,3) 의 rgb ndarray로 리턴\n",
    "    '''\n",
    "    resp = request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0\n",
    "    return image\n",
    "    #return cv2.resize(image, (512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d834668b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d834668b",
    "outputId": "5b4148ab-0b9d-414b-be4a-86f95dcabf93",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 5: Genomic context scheme of Smr45C and its closest homologues in other organisms. The αr45 RNA genes are represented by red arrows and the flanking ORFs by arrows on different colors depending on their product function (legend). Numbers indicate the αr45 RNA gene's and flanking ORFs coordinates in each organism genome database. The gene strand is represented with the file direction. On the left of the figure identification names are used which correspond to a certain organism: αr45_Smr45C = Sinorhizobium meliloti 1021 (NC_003047), αr45_Smedr45C = Sinorhizobium medicae WSM419 chromosome (NC_009636), αr45_Sfr45C = Sinorhizobium fredii NGR234 chromosome (NC_012587), αr45_Atr45C = Agrobacterium tumefaciens str. C58 chromosome linear (NC_003063), αr45_ReCIATr45C = Rhizobium etli CIAT 652 (NC_010994), αr45_Arr45C = Agrobacterium radiobacter K84 chromosome 1 (NC_011985), αr45_Rlt2304r45C = Rhizobium leguminosarum bv. trifolii WSM2304 (NC_011369), αr45_Avr45C = Agrobacterium vitis S4 chromosome 1 (NC_011989), αr45_Rlvr45C = Rhizobium leguminosarum bv. viciae 3841 (NC_008380), αr45_Rlt1325r45C = Rhizobium leguminosarum bv. trifolii WSM1325 (NC_012850), αr45_ReCFNr45C = Rhizobium etli CFN 42 (NC_007761), αr45_Mlr45C = Mesorhizobium loti MAFF303099 chromosome (NC_002678), αr45_Mcr45C = Mesorhizobium ciceri biovar biserrulae WSM1271 chromosome (NC_014923), αr45_Bcr45CII = Brucella canis ATCC 23365 chromosome II (NC_010104), αr45_Bs23445r45CII = Brucella suis ATCC 23445 chromosome II (NC_010167), αr45_Bm16Mr45CII = Brucella melitensis bv. 1 str. 16M chromosome II (NC_003318), αr45_BaS19r45CII = Brucella abortus S19 chromosome 2 (NC_010740), αr45_Bm23457r45CII = Brucella melitensis ATCC 23457 chromosome II (NC_012442), αr45_Bs1330r45CII = Brucella suis 1330 chromosome II (NC_004311), αr45_Ba19941r45CII = Brucella abortus bv. 1 str. 9-941 chromosome II (NC_006933), αr45_Bmar45CII = Brucella melitensis biovar Abortus 2308 chromosome II (NC_007624), αr45_Bor45CII = Brucella ovis ATCC 25840 chromosome II (NC_009504), αr45_Bmir45CII = Brucella microti CCM 4915 chromosome 2 (NC_013118), αr45_Oar45C = Ochrobactrum anthropi ATCC 49188 chromosome 2 (NC_009668), αr45_MsBNCr45C = Mesorhizobium sp. BNC1 (NC_008254), αr45_Bahr45C = Bartonella henselae str. Houston-1 (NC_005956), αr45_Bacr45C = Bartonella clarridgeiae 73 (NC_014932), αr45_Batr45C = Bartonella tribocorum CIP 105476 (NC_010161), αr45_Baqr45C = Bartonella quintana str. Toulouse (NC_005955), αr45_Babr45C = Bartonella bacilliformis KC583 (NC_008783), αr45_Bagr45C = Bartonella grahamii as4aup (NC_012846), αr45_Ac571r45C = Azorhizobium caulinodans ORS 571 (NC_009937), αr45_Stnr45C = Starkeya novella DSM 506 chromosome (NC_014217), αr45_Xar45C = Xanthobacter autotrophicus Py2 chromosome (NC_009720), αr45_Mesr45C = Methylocella silvestris BL2 chromosome (NC_011666), αr45_Beir45C = Beijerinckia indica subsp. indica ATCC 9039 chromosome (NC_010581), αr45_Rhpr45C = Rhodopseudomonas palustris BisA53 chromosome (NC_008435).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "imshow() missing required argument 'mat' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bf/1c7mkbrd2m73f4h1lx6h77pw0000gn/T/ipykernel_1102/2847495756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: imshow() missing required argument 'mat' (pos 2)"
     ]
    }
   ],
   "source": [
    "# 가장 긴 캡션과 그의 이미지 출력해보기 (모델상에서는 실행안해도됨.)\n",
    "from google.colab.patches import cv2_imshow\n",
    "long_caption = max(df['caption_title_and_reference_description'], key = lambda x: len(x))\n",
    "a = df[df['caption_title_and_reference_description'] == long_caption ]['image_url']\n",
    "#a = df[df['caption_title_and_reference_description'] == 'Downtown Deer Park']['image_url']\n",
    "x = url_to_image(str(a.iloc[0]))\n",
    "print(long_caption)\n",
    "cv2.imshow(x)\n",
    "print(x.shape)\n",
    "print(len(long_caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3934e7",
   "metadata": {
    "id": "ba3934e7"
   },
   "source": [
    "# 전처리\n",
    "> 위에서 확인했듯, caption 이 제일 긴 행에 대해  \n",
    "> 1. 이미지가 너무 크다(7992,1080). >  512,512 로 압축하게 되면 심각하게 찌그러 질것,\n",
    "> 2. caption이 너무 길다. 3011자.\n",
    "\n",
    "> 해결방법\n",
    "> 1. caption이 너무 긴 행(100자 이상)은 삭제.  \n",
    "> 2. image 파일이 너무 큰(가로 세로 비율이 2:1 혹은 1:2 를 초과) 경우는 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "Igvb1YUqaJDT",
   "metadata": {
    "id": "Igvb1YUqaJDT"
   },
   "outputs": [],
   "source": [
    "# 위 url_to_image 다시 정의\n",
    "def url_to_image(url):\n",
    "    '''\n",
    "    url 에서 이미지를 추출하여, (512,512,3) 의 rgb ndarray로 리턴\n",
    "    '''\n",
    "    resp = request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0\n",
    "    if type(image) == type(None):\n",
    "        return np.array([None])\n",
    "    image = image/255.0\n",
    "    if ( image.shape[0]/image.shape[1] > 2 ) or ( image.shape[0]/image.shape[1] < 1/2 ):  # 가로세로 비율이 1:2, 2:1을 벗어난다면\n",
    "        return np.array([None])\n",
    "    else:\n",
    "        return cv2.resize(image, (299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d96f146",
   "metadata": {
    "id": "6d96f146"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "(299, 299, 3)\n",
      "91\n",
      "<start>  mv agusta  raid <end>\n"
     ]
    }
   ],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for i,j in enumerate((df.iloc[:100]['image_url'])):\n",
    "    try:\n",
    "        temp = url_to_image(j)\n",
    "    :\n",
    "        pass\n",
    "    if None in temp:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "          X_train.append(temp)\n",
    "          y_train.append( df.iloc[i]['caption_title_and_reference_description'] )\n",
    "        except:\n",
    "          pass\n",
    "print(len(X_train))\n",
    "print(X_train[2].shape)\n",
    "print(len(y_train))\n",
    "print(y_train[2])\n",
    "\n",
    "# png 파일일 경우 libpng경고가 나오는데 무시해도 좋을듯하다.\n",
    "# srv 파일의 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3745efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet 가중치를 사용하여 특성추출\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "image_dataset = image_dataset.batch(len(X_train))\n",
    "\n",
    "for img in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "14a01efa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14a01efa",
    "outputId": "0ac09a49-6d11-4c56-8e09-f40eca806153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> erskine river at lorne <end>\n",
      "91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2, 81, 82,  8, 83,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train 은 캡션 문장인데, 토크나이저를 통해 문장들을 단어별 벡터화 해준다. (cap_vector)\n",
    "\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&*+.-;?@[]^`{}~ ')\n",
    "tokenizer.fit_on_texts(y_train)\n",
    "train_seqs = tokenizer.texts_to_sequences(y_train)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "print(y_train[3])\n",
    "print(len(cap_vector))\n",
    "cap_vector[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e22ce3",
   "metadata": {
    "id": "e5e22ce3"
   },
   "source": [
    "# 전역변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70c3d07c",
   "metadata": {
    "id": "70c3d07c"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 512\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(X_train) // BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aee54e",
   "metadata": {
    "id": "d5aee54e"
   },
   "source": [
    "# optimizer, loss func.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "95481c3d",
   "metadata": {
    "id": "95481c3d"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d589303",
   "metadata": {
    "id": "1d589303"
   },
   "source": [
    "# model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db640294",
   "metadata": {
    "id": "db640294"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    '''\n",
    "    W1, W2, V 는 학습가능한 가중치벡터\n",
    "    '''\n",
    "    super(BahdanauAttention, self).__init__()   # 부모클래스 (tf.keras.Model.init()사용)\n",
    "    self.W1 = tf.keras.layers.Dense(units)      # units(전역) 수 의 node를 갖는 W1 가중치\n",
    "    self.W2 = tf.keras.layers.Dense(units)      # 위와 동 w2 가중치\n",
    "    self.V = tf.keras.layers.Dense(1)           # V가중치는 하나로.\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    '''\n",
    "    feautures : 이미지의 피쳐맵\n",
    "    hidden : hidden state\n",
    "    '''\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4826bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "34c8e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "image_dataset = image_dataset.batch(len(X_train))\n",
    "\n",
    "for img in image_dataset:\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a140632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([91, 64, 2048])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27003f76",
   "metadata": {
    "id": "27003f76"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "n6zPA3vDezmt",
   "metadata": {
    "id": "n6zPA3vDezmt"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    x = self.fc1(output)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    x = self.fc2(x)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28da6c1",
   "metadata": {
    "id": "d28da6c1"
   },
   "source": [
    "# trian step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca31e52",
   "metadata": {
    "id": "cca31e52",
    "outputId": "9f42e20c-7126-4327-b54f-e7da31c7ea11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 21:27:42.025145: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-28 21:27:42.026399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc40270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ad928",
   "metadata": {
    "id": "921ad928"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_lossa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d691eb",
   "metadata": {
    "id": "06d691eb"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
